servingEngineSpec:
  runtimeClassName: "nvidia"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"

  modelSpec:
    - name: "llama3"

      repository: "vllm/vllm-openai"
      tag: "v0.14.1"          # PIN â€” no latest

      # Passed positionally to: vllm serve <modelURL>
      modelURL: "/data/gpt-neox-20b"
      replicaCount: 1

      requestCPU: 10
      requestMemory: "16Gi"
      requestGPU: 1

      pvcStorage: "200Gi"
      pvcAccessMode: ["ReadWriteMany"]

      # OFFLINE MODE (this is enough)
      env:
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: TRANSFORMERS_OFFLINE
          value: "1"
        - name: HF_DATASETS_OFFLINE
          value: "1"
        - name: HF_HUB_DISABLE_TELEMETRY
          value: "1"
        - name: HF_TOKEN
          value: ""
        - name: HF_HOME
          value: "/data/.cache"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"

      vllmConfig:
        maxModelLen: 8192
        dtype: "float16"
        extraArgs:
          # tokenizer override is optional but safe
          - "--tokenizer"
          - "/data/gpt-neox-20b"

          - "--gpu-memory-utilization"
          - "0.95"

          - "--disable-log-requests"

      hf_token: ""

routerSpec:
  repository: "941284018955.dkr.ecr.il-central-1.amazonaws.com/vllm/lmstack-router"
  tag: "latest"
  imagePullPolicy: "Always"
  resources:
    requests:
      cpu: 500m
      memory: 16Gi
    limits:
      cpu: 2000m
      memory: 32Gi
