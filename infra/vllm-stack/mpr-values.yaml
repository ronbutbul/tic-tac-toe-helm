servingEngineSpec:
  runtimeClassName: "nvidia"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"

  modelSpec:
    - name: "llama3"

      repository: "vllm/vllm-openai"
      tag: "v0.14.1"          # PIN â€” no latest

      # Passed positionally to: vllm serve <modelURL>
      modelURL: "/data/Ministral-3-8B-Instruct-2512"
      replicaCount: 1

      requestCPU: 10
      requestMemory: "16Gi"
      requestGPU: 1

      pvcStorage: "200Gi"
      pvcAccessMode: ["ReadWriteMany"]

      # OFFLINE MODE (this is enough)
      env:
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: TRANSFORMERS_OFFLINE
          value: "1"
        - name: HF_DATASETS_OFFLINE
          value: "1"
        - name: HF_HUB_DISABLE_TELEMETRY
          value: "1"
        - name: HF_TOKEN
          value: ""
        - name: HF_HOME
          value: "/data/.cache"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"

      vllmConfig:
        maxModelLen: 1024
        dtype: "float16"
        extraArgs:
          # tokenizer override is optional but safe
          - "--tokenizer"
          - "/data/Ministral-3-8B-Instruct-2512"

          - "--gpu-memory-utilization"
          - "0.95"

          - "--disable-log-requests"

      hf_token: ""

routerSpec:
  repository: "941284018955.dkr.ecr.il-central-1.amazonaws.com/vllm/lmstack-router:latest"
  tag: "latest"
  imagePullPolicy: "Always"
