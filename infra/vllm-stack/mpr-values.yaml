servingEngineSpec:
  runtimeClassName: "nvidia"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"

  modelSpec:
    - name: "llama3"

      repository: "vllm/vllm-openai"
      tag: "v0.14.1"          # PIN â€” no latest
      imagePullPolicy: "Always" 

      # Passed positionally to: vllm serve <modelURL>
      modelURL: "/data/gpt-oss-20b"
      replicaCount: 1

      requestCPU: 10
      requestMemory: "16Gi"
      requestGPU: 1
      storageClass: efs-sc-vllm
      pvcStorage: "200Gi"
      pvcAccessMode: ["ReadWriteMany"]
      lmcacheConfig: 
        enabled: true
        cpuOffloadingBufferSize: "10"
      env:
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: TRANSFORMERS_OFFLINE
          value: "1"
        - name: HF_DATASETS_OFFLINE
          value: "1"
        - name: HF_HUB_DISABLE_TELEMETRY
          value: "1"
        - name: HF_TOKEN
          value: ""
        - name: HF_HOME
          value: "/data/.cache"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"    
        - name: TIKTOKEN_ENCODINGS_BASE
          value: "/data/gpt-oss-20b/tiktoken_encodings"
      vllmConfig:
        maxModelLen: 40192
        dtype: "auto"
        extraArgs:
          # tokenizer override is optional but safe
          - "--tokenizer"
          - "/data/gpt-oss-20b"
          - "--gpu-memory-utilization"
          - "0.95"
          - "--enable-auto-tool-choice"
          - "--tool-call-parser"
          - "openai"
      hf_token: ""

routerSpec:
  repository: "941284018955.dkr.ecr.il-central-1.amazonaws.com/vllm/lmstack-router"
  tag: "latest"
  imagePullPolicy: "Always"
  resources:
    requests:
      cpu: 500m
      memory: 16Gi
    limits:
      cpu: 2000m
      memory: 32Gi
