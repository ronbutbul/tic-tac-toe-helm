servingEngineSpec:
  runtimeClassName: "nvidia"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"

  modelSpec:
    - name: "llama3"

      # repository: "vllm/vllm-openai"
      # tag: "v0.14.1"          # PIN â€” no latest
      # imagePullPolicy: "Always" 
      repository: "lmcache/vllm-openai"
      tag: "latest-nightly"
      # Passed positionally to: vllm serve <modelURL>
      modelURL: "/data/gpt-oss-20b"
      replicaCount: 1

      requestCPU: 10
      requestMemory: "16Gi"
      requestGPU: 1
      storageClass: efs-sc-vllm
      pvcStorage: "200Gi"
      pvcAccessMode: ["ReadWriteMany"]
      lmcacheConfig: 
        enabled: true
        enableController: true
        cpuOffloadingBufferSize: "10"
        logLevel: "DEBUG"
      env:
        - name: HF_HUB_OFFLINE
          value: "1"
        - name: TRANSFORMERS_OFFLINE
          value: "1"
        - name: HF_DATASETS_OFFLINE
          value: "1"
        - name: HF_HUB_DISABLE_TELEMETRY
          value: "1"
        - name: HF_TOKEN
          value: ""
        - name: HF_HOME
          value: "/data/.cache"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: VLLM_ALLOW_LONG_MAX_MODEL_LEN
          value: "1"    
        - name: TIKTOKEN_ENCODINGS_BASE
          value: "/data/gpt-oss-20b/tiktoken_encodings"
      vllmConfig:
        # enableChunkedPrefill: false
        enablePrefixCaching: true
        maxModelLen: 40192
        dtype: "auto"
        extraArgs:
          # tokenizer override is optional but safe
          - "--tokenizer"
          - "/data/gpt-oss-20b"
          - "--gpu-memory-utilization"
          - "0.95"
          - "--enable-auto-tool-choice"
          - "--tool-call-parser"
          - "openai"
      hf_token: ""
      initContainer:
        name: "wait-for-cache-server"
        image: lmcache/vllm-openai:latest-nightly
        command: ["/bin/sh", "-c"]
        args:
          - |
            timeout 60 bash -c '
            while true; do
              /opt/venv/bin/python3 /workspace/LMCache/examples/kubernetes/health_probe.py $(RELEASE_NAME)-cache-server-service $(LMCACHE_SERVER_SERVICE_PORT) && exit 0
              echo "Waiting for LMCache server..."
              sleep 2
            done'

cacheserverSpec:
  # -- Number of replicas
  replicaCount: 1

  # -- Container port
  containerPort: 8080

  # -- Service port
  servicePort: 81

  # -- Serializer/Deserializer type
  serde: "naive"

  # -- Cache server image (reusing the vllm image)
  repository: "lmcache/vllm-openai"
  tag: "latest-nightly"

  # TODO (Jiayi): please adjust this once we have evictor
  # -- router resource requests and limits
  resources:
    requests:
      cpu: "4"
      memory: "8G"
    limits:
      cpu: "4"
      memory: "10G"

  # -- (Optional) Liveness probe for the cache server, available in vllm-openai:latest-nightly
  # livenessProbe:
  #   initialDelaySeconds: 30
  #   periodSeconds: 10
  #   failureThreshold: 3
  #   timeoutSeconds: 5

  # -- Customized labels for the cache server deployment
  labels:
    environment: "cacheserver"
    release: "cacheserver"

routerSpec:
  repository: "941284018955.dkr.ecr.il-central-1.amazonaws.com/vllm/lmstack-router"
  tag: "latest"
  imagePullPolicy: "Always"
  resources:
    requests:
      cpu: 500m
      memory: 16Gi
    limits:
      cpu: 2000m
      memory: 32Gi
