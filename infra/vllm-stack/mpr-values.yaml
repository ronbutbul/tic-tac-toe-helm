servingEngineSpec:
  runtimeClassName: "nvidia"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"
  modelSpec:
  - name: "llama3"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "/models/gpt-neox-20b"
    replicaCount: 1
    requestCPU: 10
    requestMemory: "16Gi"
    requestGPU: 1
    pvcStorage: "200Gi"
    pvcAccessMode: ["ReadWriteMany"]

    env:
      - name: HF_HUB_OFFLINE
        value: "1"
      - name: TRANSFORMERS_OFFLINE
        value: "1"
      - name: HF_HOME
        value: "/models/.cache"
      - name: VLLM_LOGGING_LEVEL
        value: "DEBUG"
      - name: HF_TOKEN
        value: ""
    # vllmConfig:
    #   enableChunkedPrefill: false
    #   enablePrefixCaching: false
    #   maxModelLen: 16384
    #   dtype: "bfloat16"
    #   extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]

    vllmConfig:
      maxModelLen: 2048
      dtype: "float16"
      extraArgs:
        # tokenizer override is optional; keep if your model needs it
        - "--tokenizer"
        - "/models/gpt-neox-20b"

        - "--load-format"
        - "auto"

        - "--gpu-memory-utilization"
        - "0.85"

        - "--disable-log-requests"

    hf_token: ""
routerSpec:
  # -- The docker image of the router. The following values are defaults:
  repository: "ronbutbul135/lmstack-router"
  tag: "latest"
  imagePullPolicy: "Always"
