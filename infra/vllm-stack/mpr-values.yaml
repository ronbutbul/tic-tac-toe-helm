servingEngineSpec:
  runtimeClassName: "nvidia-cdi"
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Equal"
  modelSpec:
  - name: "llama3"
    repository: "vllm/vllm-openai"
    tag: "latest"
    modelURL: "/models/gpt-neox-20b"
    replicaCount: 1
    requestCPU: 10
    requestMemory: "16Gi"
    requestGPU: 1
    extraVolumes:
      - name: toolkit-root
        hostPath:
          path: /run/nvidia/toolkit
          type: Directory

    # âœ… THEN: mount it in the init container
    initContainer:
      name: fix-nvidia-perms
      image: alpine:3.19
      command:
        - sh
        - -c
        - |
          chmod -R 755 /run/nvidia/toolkit
      mountPvcStorage: false
      extraVolumeMounts:
        - name: toolkit-root
          mountPath: /run/nvidia/toolkit

    pvcStorage: "200Gi"
    pvcAccessMode: ["ReadWriteMany"]

    vllmConfig:
      enableChunkedPrefill: false
      enablePrefixCaching: false
      maxModelLen: 16384
      dtype: "bfloat16"
      extraArgs: ["--disable-log-requests", "--gpu-memory-utilization", "0.8"]

    hf_token: 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
routerSpec:
  # -- The docker image of the router. The following values are defaults:
  repository: "ronbutbul135/lmstack-router"
  tag: "latest"
  imagePullPolicy: "Always"
